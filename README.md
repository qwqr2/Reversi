# Reversi AI - 黑白棋人工智能

本项目实现了一个结合了传统搜索算法与深度强化学习的黑白棋（Reversi/Othello）AI。玩家可以选择与不同类型的 AI 对战，包括基于 Alpha-Beta 剪枝的传统 AI 和基于策略价值网络的深度学习 AI。

## 特点

*   **多种 AI 对手**：
    *   **人类玩家**：手动操作。
    *   **传统算法 AI (`ChessAIPlayer`)**：基于 Alpha-Beta 剪枝搜索和静态棋局评估函数。
    *   **神经网络 AI (`AIPlayerplus`)**：采用策略与价值网络 (Policy-Value Network)，结合蒙特卡洛树搜索 (MCTS) 进行决策，通过自我对弈进行强化学习训练。
*   **模型训练**：提供 `train.py` 脚本，通过自我对弈 (self-play) 收集数据并训练神经网络模型。
*   **灵活对战**：
    *   `main.py`：在命令行进行人机或 AI 间的单局对战。
    *   `selfplay.py`：进行 AI 间的批量自动对战，方便评估模型性能和胜率。

## 算法分析与比较

本项目实现了三种不同的AI算法，各有特点和适用场景：

### 1. 传统Alpha-Beta剪枝搜索 (chess.py)

**核心特点：**
- 基于极小极大搜索与Alpha-Beta剪枝，大幅减少搜索空间
- 采用迭代加深策略(2,3,4,6层)，平衡搜索深度与时间开销
- 利用转置表(Transposition Table)缓存已搜索局面，避免重复计算
- 使用静态评估函数，特别重视角落(2000分)和避免危险位置(-400分)
- 根据游戏阶段自适应调整评估策略

**优势：**
- 确定性强，在相同条件下总是做出相同决策
- 计算资源需求低，不需要GPU
- 搜索深度可控，根据时间限制调整
- 适合已知评估函数的棋类游戏

**劣势：**
- 依赖人工设计的评估函数，质量上限受限于设计者的专业知识
- 在分支因子大的游戏中效率降低
- 难以像学习型AI那样持续进步

### 2. 纯蒙特卡洛树搜索 (mcts.py)

**核心特点：**
- 基于四个阶段(选择-扩展-模拟-反向传播)的标准MCTS实现
- 使用UCB1公式平衡探索与利用
- 通过大量随机模拟评估局面价值
- 搜索深度不固定，取决于随机模拟的深度

**优势：**
- 不依赖领域特定知识，通用性强
- 可适用于信息不完全和随机性游戏
- 通过增加模拟次数可持续提升性能
- 探索宽度更广，不易陷入局部最优

**劣势：**
- 模拟次数需求大，计算开销较高
- 纯随机模拟效率低，需要大量迭代才能获得准确评估
- 在游戏后期表现不如传统搜索算法

### 3. 神经网络增强的蒙特卡洛树搜索 (mcts_plus.py)

**核心特点：**
- 类似AlphaZero的方法，结合神经网络与MCTS。神经网络基础原理参考于https://zhuanlan.zhihu.com/p/32089487
- 使用策略-价值神经网络替代随机模拟，提供位置评估和行动概率
- 采用PUCT公式指导树搜索，结合先验概率与UCB
- 支持自我对弈训练模式，能够通过强化学习不断进步
- 在自我对弈中使用Dirichlet噪声增加探索性

**优势：**
- 性能潜力最大，通过持续训练可以达到超越人类的水平
- 在相同模拟次数下比纯MCTS表现更好
- 结合了神经网络的泛化能力与树搜索的战术计算能力
- 不依赖人工设计的评估函数，可自主发现优秀策略

**劣势：**
- 需要大量计算资源进行训练(GPU)
- 依赖高质量的训练数据
- 模型调参复杂，效果受网络结构和超参数影响
- 黑盒特性导致决策过程不如传统算法透明

## 环境配置

*   Python (推荐 3.7+)
*   PyTorch (用于神经网络模型)
*   NumPy

你可以使用 pip 安装必要的库：
```bash
pip install torch numpy
```

## 文件结构说明

```
Reversi/
├── board.py            # 定义棋盘状态、规则和基本操作
├── game.py             # 定义游戏流程控制和玩家交互逻辑
├── player.py           # 定义不同类型的玩家 (人类, 传统AI, 神经网络AI)
├── mcts_plus.py        # 实现结合神经网络的蒙特卡洛树搜索 (MCTS)
├── policy_value_net.py # 定义神经网络模型 (策略价值网络)
├── train.py            # 神经网络模型的训练脚本
├── selfplay.py         # AI 自动对战脚本，用于评估和测试
├── main.py             # 命令行交互式对战主程序
├── chess.py           # 传统 AI 的核心算法 (评估函数, Alpha-Beta剪枝)
├── current_policy.model # 当前训练的神经网络模型文件
├── best_policy.model    # 训练过程中表现最佳的神经网络模型文件
└── README.md           # 项目说明文件
```
current_policy.model和best_policy.model均为模拟1000次，训练50次得出的模型

## 使用说明

### 1. 训练神经网络模型

如果你想从头开始训练或继续训练神经网络模型：

```bash
python train.py
```

*   训练脚本会通过自我对弈生成数据，并使用这些数据更新神经网络。
*   训练过程中会定期评估模型性能，并保存当前模型 (`current_policy.model`) 和迄今为止表现最佳的模型 (`best_policy.model`)。
*   你可以通过修改 `train.py` 中的参数 (如学习率、MCTS 模拟次数、训练轮数等) 来调整训练过程。

### 2. 进行命令行对战

运行 `main.py` 与 AI 或让 AI 之间进行对战：

```bash
python main.py
```

程序会提示你为黑棋和白棋选择玩家类型：
1.  人类玩家
2.  神经网络 AI (`AIPlayerplus`)
3.  传统算法 AI (`ChessAIPlayer`)

根据提示输入选择，并配置相应 AI 的参数 (如模型路径、MCTS 模拟次数、搜索深度等)。


